{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1dO57VRdOBU",
        "colab_type": "text"
      },
      "source": [
        "**Objective :**\n",
        "Create the neural network model by using tensorflow so as to predict the canser servival at 3 Year'.\n",
        "\n",
        "**Steps :**\n",
        "1. Review data by checking the 'null value' in data. (In fact, I reviewed dataset on Microsoft Excel so as to see overall data first.)\n",
        "2. There are 849 features(columns) in dataset and I need to predict the answer in YES/NO, so I decide to use 'Neuro Network Prediction Model'\n",
        "3. Check null data & cleaning data.\n",
        "4. Balancing input data.\n",
        "5. Shuffle input and target data.\n",
        "6. Split dataset for training, testing and validation data.\n",
        "7. Export training, testing and validation data for Tensorflow format.\n",
        "8. Run model.\n",
        "9. Check the accuracy of model.\n",
        "10. Summarize.\n",
        "\n",
        "---\n",
        "\n",
        "**Let start!!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uG5ndJ1Bht3u",
        "colab_type": "text"
      },
      "source": [
        "Mount Google Drive and connect colab with google drive to make sure I can load data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmiOiOesCc11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "ac94d0ef-8d00-4c4a-87bf-407ed8749ccb"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWDH3uN8iCCz",
        "colab_type": "text"
      },
      "source": [
        "Load data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSrjSpZFguxY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a615148d-aad0-4b08-85db-5736fbf8120e"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn import preprocessing\n",
        "\n",
        "path='/content/drive/My Drive/Chula UTC AI Academy /Chula UTC AI Academy - Practical Exam/Problem2/CancerRadiomicsKaggle_v2_050620.csv'\n",
        "\n",
        "data_ground_truth = np.loadtxt(path, delimiter = ',' ,skiprows=1)\n",
        "print('Size or original dataset : ' + str(data_ground_truth.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size or original dataset : (197, 852)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvPLpWaTlkCD",
        "colab_type": "text"
      },
      "source": [
        "Preprocess the data. I split raw data to 2 groups,'input' & 'target' data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd_JT-V3lgvE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "7cfe5b39-f86f-4198-9684-30edbf55be18"
      },
      "source": [
        "# Remove record that has 'Patient Status at 3 Year' status == -1 (Unknow status.)\n",
        "data_ground_truth_without_unknow_status = data_ground_truth[np.logical_and(data_ground_truth[:,9]>=0, data_ground_truth[:,9]<=1)]\n",
        "print('Size or original dataset after remove record that has \\'Patient Status at 3 Year\\' status == unkown : ' + str(data_ground_truth_without_unknow_status.shape))\n",
        "\n",
        "# Delele colum 'Patient ID', 'Time to Event', 'Patient Status at 3 Year'\n",
        "unscaled_input_data_ground_truth = np.delete(data_ground_truth_without_unknow_status, [0,7,9], axis=1)\n",
        "print('Size of input data after delete 3 columns : '+ str(unscaled_input_data_ground_truth.shape))\n",
        "\n",
        "# All target data\n",
        "targets_data_ground_truth = data_ground_truth_without_unknow_status[:, 9]\n",
        "print('Size of target data : ' + str(targets_data_ground_truth.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size or original dataset after remove record that has 'Patient Status at 3 Year' status == unkown : (195, 852)\n",
            "Size of input data after delete 3 columns : (195, 849)\n",
            "Size of target data : (195,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1lJiE1QpdBX",
        "colab_type": "text"
      },
      "source": [
        "Balancing input & target data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY-hpPythkII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9aa5f580-1215-4682-c754-587d45edab71"
      },
      "source": [
        "# Check Number of death patient in target data.\n",
        "number_death_targets = int(np.sum(targets_data_ground_truth))\n",
        "print(\"Number of death targets : \" + str(number_death_targets))\n",
        "number_zero_targets_counter = 0\n",
        "indices_to_remove = []\n",
        "\n",
        "for i in range(targets_data_ground_truth.shape[0]):\n",
        "  # Check only target == 0 (Not Death)\n",
        "  if targets_data_ground_truth[i] == 0:\n",
        "    number_zero_targets_counter += 1\n",
        "    # If number of 0 target is morethan number of 1.\n",
        "    if number_zero_targets_counter > number_death_targets:\n",
        "        # Keep id that need to remove into indices_to_remove array\n",
        "        indices_to_remove.append(i)\n",
        "        #print(targets_data_ground_truth[i])\n",
        "\n",
        "# Delete record in 'unscaled_input_data_ground_truth' and 'targets_data_ground_truth' that has the same id in indices_to_remove array.\n",
        "unscaled_input_equal_priors = np.delete(unscaled_input_data_ground_truth, indices_to_remove, axis=0)\n",
        "targets_equal_priors = np.delete(targets_data_ground_truth, indices_to_remove, axis=0)\n",
        "\n",
        "print(\"Size of unscale input data : \" + str(unscaled_input_equal_priors.shape))\n",
        "print(\"Size of target data : \" + str(targets_equal_priors.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of death targets : 31\n",
            "Size of unscale input data : (62, 849)\n",
            "Size of target data : (62,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94dupSk1vAE9",
        "colab_type": "text"
      },
      "source": [
        "Scale input data by implementing scale feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg_unZg_k4Yu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "6fad66c4-a0b4-48e7-cf31-339a9db0b33f"
      },
      "source": [
        "scaled_inputs = preprocessing.scale(unscaled_input_equal_priors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
            "  warnings.warn(\"Numerical issues were encountered \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yck2HOom0BsX",
        "colab_type": "text"
      },
      "source": [
        "Shuffle input and target data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Igez-svSjK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "d5547b67-8161-4229-f981-c9d2c8ac447d"
      },
      "source": [
        "# Create shuffle index\n",
        "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
        "# Random shuffle\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "# Apply index into input&target array.\n",
        "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
        "shuffled_targets = targets_equal_priors[shuffled_indices]\n",
        "\n",
        "print(shuffled_inputs)\n",
        "print(shuffled_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-4.38529010e-01  1.42045487e+00  3.71811446e-01 ... -3.14473460e-01\n",
            "  -7.04995948e-01  7.76010159e-01]\n",
            " [-4.38529010e-01 -8.48894240e-01  1.29390383e+00 ...  7.29483232e-02\n",
            "  -3.14302443e-01 -4.20828115e-01]\n",
            " [-4.38529010e-01 -2.37915633e-01 -5.50280940e-01 ... -2.79765531e-01\n",
            "  -7.05481516e-01 -1.12919295e-01]\n",
            " ...\n",
            " [-4.38529010e-01  2.39323418e-02 -5.50280940e-01 ...  9.18079957e-02\n",
            "  -1.59142102e-02 -8.83393773e-01]\n",
            " [-4.38529010e-01  5.47628291e-01 -1.47237333e+00 ...  6.90576675e-02\n",
            "  -6.95271603e-01 -1.41654643e-01]\n",
            " [-4.38529010e-01  2.39323418e-02  1.29390383e+00 ... -6.57575825e-01\n",
            "  -6.57193403e-04  9.49592036e-03]]\n",
            "[1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0.\n",
            " 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1.\n",
            " 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4a8T6Ge4a5c",
        "colab_type": "text"
      },
      "source": [
        "Devide dataset into train, validation and test for preventing the overfitting.\n",
        "![alt text](https://www.aoddy.com/wp-content/uploads/2020/05/Split_data.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kme47e1O2GvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "c5ce0350-eb67-41b8-81a5-b1758faead29"
      },
      "source": [
        "# Check number of input data\n",
        "samples_count = shuffled_inputs.shape[0]\n",
        "print(\"Total sample data : \" + str(samples_count))\n",
        "\n",
        "# I split data to \n",
        "#   80% for training\n",
        "#   10% for validating\n",
        "#   10% for testing\n",
        "\n",
        "train_samples_count = int(0.8*samples_count)\n",
        "validation_samples_count = int(0.1*samples_count)\n",
        "testing_samples_count = samples_count - train_samples_count - validation_samples_count\n",
        "\n",
        "print(\"Training sample number : \" + str(train_samples_count))\n",
        "print(\"Validating sample number : \" + str(validation_samples_count))\n",
        "print(\"Testing sample number : \" + str(testing_samples_count))\n",
        "\n",
        "# Split shuffled input & target data\n",
        "train_inputs = shuffled_inputs[: train_samples_count]\n",
        "validation_inputs = shuffled_inputs[train_samples_count : train_samples_count + validation_samples_count]\n",
        "test_inputs = shuffled_inputs[train_samples_count + validation_samples_count :]\n",
        "\n",
        "train_targets = shuffled_targets[: train_samples_count]\n",
        "validation_targets = shuffled_targets[train_samples_count : train_samples_count + validation_samples_count]\n",
        "test_targets = shuffled_targets[train_samples_count + validation_samples_count : ]\n",
        "\n",
        "print(\"Double check size of input data : \" + str(train_inputs.shape[0] + validation_inputs.shape[0] + test_inputs.shape[0]))\n",
        "print(\"Double check size of target data : \" + str(train_targets.shape[0] + validation_targets.shape[0] + test_targets.shape[0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sample data : 62\n",
            "Training sample number : 49\n",
            "Validating sample number : 6\n",
            "Testing sample number : 7\n",
            "Double check size of input data : 62\n",
            "Double check size of target data : 62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N0gP9Hs7HA4",
        "colab_type": "text"
      },
      "source": [
        "Save training, validation and testing data for tensorfow format (.npz)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiFYmO-l4w8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.savez('CancerRadiomics_data_train', inputs=train_inputs, targets=train_targets)\n",
        "np.savez('CancerRadiomics_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
        "np.savez('CancerRadiomics_data_test', inputs=test_inputs, targets=test_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqa--mfhBiOP",
        "colab_type": "text"
      },
      "source": [
        "Load training, validation and testing data from files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5udOnZ2YAOyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "npz = np.load('CancerRadiomics_data_train.npz')\n",
        "train_inputs = npz['inputs'].astype(np.float)\n",
        "train_targets = npz['targets'].astype(np.int)\n",
        "\n",
        "npz = np.load('CancerRadiomics_data_validation.npz')\n",
        "validation_inputs = npz['inputs'].astype(np.float)\n",
        "validation_targets = npz['targets'].astype(np.int)\n",
        "\n",
        "npz = np.load('CancerRadiomics_data_test.npz')\n",
        "test_inputs = npz['inputs'].astype(np.float)\n",
        "test_targets = npz['targets'].astype(np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMut9310B3Iy",
        "colab_type": "text"
      },
      "source": [
        "Run model. - The input size is number of features and output size is 2 because the answers is YES or NO."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXnWV9XXApqG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "470a3137-2993-4710-d459-c26d12330600"
      },
      "source": [
        "input_size = 849\n",
        "output_size = 2\n",
        "hidden_layer_size = 566\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
        "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
        "                            ])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 100\n",
        "max_epochs = 100\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "\n",
        "model.fit(train_inputs,\n",
        "          train_targets,\n",
        "          batch_size = batch_size,\n",
        "          epochs = max_epochs,\n",
        "          callbacks=[early_stopping],\n",
        "          validation_data = (validation_inputs, validation_targets),\n",
        "          verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 - 0s - loss: 1.0142 - accuracy: 0.4898 - val_loss: 2.4880 - val_accuracy: 0.3333\n",
            "Epoch 2/100\n",
            "1/1 - 0s - loss: 1.5885 - accuracy: 0.5102 - val_loss: 0.5613 - val_accuracy: 0.5000\n",
            "Epoch 3/100\n",
            "1/1 - 0s - loss: 0.5353 - accuracy: 0.7959 - val_loss: 0.0754 - val_accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "1/1 - 0s - loss: 0.3326 - accuracy: 0.8776 - val_loss: 0.0460 - val_accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "1/1 - 0s - loss: 0.3720 - accuracy: 0.7959 - val_loss: 0.0651 - val_accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "1/1 - 0s - loss: 0.2050 - accuracy: 0.8776 - val_loss: 0.1802 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb818ae7080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BqKiSEEC02R",
        "colab_type": "text"
      },
      "source": [
        "Summarize \n",
        "The accuracy of this model is 85% but the percent loss is quit high. So if we have more ground truth data, the number of accuracy should be increase and percent loss should be reduce. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMtyDnDpAu2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f308afca-e0bd-4ca6-d64e-4e193c4f2c89"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.8571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b03B606HxsBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "793cc3bb-c66a-4584-f23a-8d47c7e6d1ca"
      },
      "source": [
        "print('\\n# Generate predictions for 3 samples')\n",
        "predictions = model.predict(test_inputs[:3])\n",
        "print('predictions shape:', predictions.shape)\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "# Generate predictions for 3 samples\n",
            "predictions shape: (3, 2)\n",
            "[[9.7574371e-01 2.4256352e-02]\n",
            " [6.4135375e-03 9.9358648e-01]\n",
            " [9.9986994e-01 1.3009210e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
